{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":22922,"sourceType":"datasetVersion","datasetId":17473}],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.ibb.co/yQ3D64X/face.jpg)\n\n# Customer churn\n\n\n\n*It is necessary to predict whether the client will leave the bank in the near future or not.\nWe have been provided with historical data on customer behavior and termination of agreements with the bank*\n","metadata":{"id":"aCaCNgXqBlA3"}},{"cell_type":"markdown","source":"## General information about data in operation and preinspection","metadata":{"id":"9fvbQR_aBlA4"}},{"cell_type":"markdown","source":"### Importing required libraries","metadata":{"id":"AtXxcBqGBlA4"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve, accuracy_score\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom catboost import Pool, cv\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","metadata":{"id":"9MmuTFM6BlA4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring Data Set","metadata":{"id":"Bu2hLpL5BlA5"}},{"cell_type":"code","source":"df = pd.read_csv('../input/bank-customer-churn-modeling/Churn_Modelling.csv')\ndf.head()","metadata":{"id":"89u_dXeuBlA5","outputId":"e593cc87-1f73-4ade-ad97-ff651b061aa7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features: \n\n- `RowNumber` - the index of the row in the data\n- `CustomerId` - unique customer identifier\n- `Surname` - surname\n- `CreditScore` - credit rating\n- `Geography` - country of residence\n- `Gender` - gender\n- `Age` - age\n- `Tenure` - how many years a person has been a client of the bank\n- `Balance` - account balance\n- `NumOfProducts` - the number of bank products used by the client\n- `HasCrCard` - availability of a credit card\n- `IsActiveMember` - client activity\n- `EstimatedSalary` - estimated salary\n\nTarget column:\n\n- `Exited` - the fact of the client's departure","metadata":{"id":"3cnVG3SSBlA6"}},{"cell_type":"markdown","source":"For our convenience, we will lower the name of the columns and bring them to the serpentine register","metadata":{"id":"Xki_f4a6BlA6"}},{"cell_type":"code","source":"df.columns = df.columns.str.lower()\ndf.columns","metadata":{"id":"E9-ItOP7BlA6","outputId":"d6b931f9-e662-4874-8530-54ad6ae59a92","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['row_number', 'customer_id', 'surname', 'creditscore', 'geography',\n       'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_crcard',\n       'isactive_member', 'estimated_salary', 'exited']\ndf.columns","metadata":{"id":"dJkfisMpBlA6","outputId":"42f785ea-23e9-486d-c4c0-7328dd4d5740","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see general information about data in work","metadata":{"id":"mpMqlgQQBlA7"}},{"cell_type":"code","source":"print(df.shape)\ndf.info()","metadata":{"id":"OPEse60xBlA7","outputId":"e1a50986-2eb3-4d1d-8287-2460a9bb0cd5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have no NaN in our set","metadata":{"id":"6k2dnVRRBlA7"}},{"cell_type":"markdown","source":"\nLet's take a look at the data types in our dataframe separately:","metadata":{"id":"HL9IY_AXBlA8"}},{"cell_type":"code","source":"df.dtypes","metadata":{"id":"uuZh1aEXBlA8","outputId":"ca0fdfe6-a349-4a5c-d556-c354cb88abb1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need `tenure` to be integre","metadata":{"id":"ZKgADEObBlA8"}},{"cell_type":"code","source":"df.describe().T","metadata":{"id":"nsdthm8LBlA8","outputId":"be927cf9-c343-4e58-b057-7598d1443069","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first glance, we do not observe anomalies in the data, significant outliers. For example, by age, the minimum age is 18 - the maximum is 92, which may be true.\n\nLet's see the target column `exited`","metadata":{"id":"qXwt8rNjBlA8"}},{"cell_type":"code","source":"df['exited'].value_counts().to_frame()","metadata":{"id":"DR5oO2btBlA8","outputId":"175f7924-879e-4842-97d0-431d3a64edef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentage of positive marks: {:.2%}'.format(df['exited'].mean()))","metadata":{"id":"PreN2TYCBlA8","outputId":"a7c3c7a0-1f68-4dcd-c589-e661d82a2277","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a class imbalance. We will train the model on the initial data, then we will try to overcome the imbalance and train again. Let's see the results later.\n","metadata":{"id":"ZBEPkvwjBlA9"}},{"cell_type":"code","source":"df.hist(bins=50, figsize=(20,15), edgecolor='black', linewidth=2)\nplt.show()","metadata":{"id":"wKK_U3eQBlA-","outputId":"28ae3508-970a-4a6b-865e-e4dff15ba725","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the process of feature engeneering for our model, it will be possible to delete three columns - `customer_id`,` row_number` and `surname`, which do not carry the payload in our case","metadata":{"id":"MKlrjmmfBlA_"}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"id":"3PTGOQpyBlA_","outputId":"7c57a34f-22f3-49a8-8952-e52df4c989b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{"id":"rMgN_6hRBlA_"}},{"cell_type":"markdown","source":"Customer churn is the loss of customers, expressed in the absence of purchases or payments over a period of time. Churn rate is extremely important for companies with a subscription and transactional business model that means recurring payments to the company.\n\nWe've previewed our dataset:\n\n- no duplicates found, no need to delete lines\n- in the process of preparing features for analysis - remove the columns `customer_id`,` row_number` and `surname`\n\n\nWe can start preparing the features","metadata":{"id":"4HG_yiTPBlA_"}},{"cell_type":"markdown","source":"## Research of task","metadata":{"id":"pq_wzUotBlA_"}},{"cell_type":"markdown","source":"We are faced with the task of classification - it is necessary to determine whether the client will leave in the near future or not. Thus, to achieve the goals of this task, I propose to use the algorithms of Logistic Regression, Random Forest and Catboost.\n\nTo evaluate the models, we will use the F1 measure (`F1 score`) (let us apply the good values is > 0.59)\n\nTo evaluate the final model, we use the ROC curve with its area (`ROC-AUC`).\n\nAs we found out, we have an imbalance of classes, accuracy does not suit us.","metadata":{"id":"h_j_B6A1BlA_"}},{"cell_type":"markdown","source":"### Features engeneering","metadata":{"id":"-jTMmUmRBlA_"}},{"cell_type":"code","source":"df.head()","metadata":{"id":"PrCfq7gDBlA_","outputId":"406d725e-99ee-4d3c-dff3-912c7540409f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's remove unnecessary features and form a new date set so as not to overwrite variables","metadata":{"id":"RiSx5KcnBlBA"}},{"cell_type":"code","source":"data = df.drop(['row_number', 'customer_id', 'surname'], axis=1).copy()\ndata.head()","metadata":{"scrolled":true,"id":"R3VVgmExBlBA","outputId":"a1c45ff5-cff1-4215-db26-9703518031db","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One-hot Encoding","metadata":{"id":"TWU1FKZjBlBA"}},{"cell_type":"markdown","source":"The categorical features `geography` and` gender` must be converted to numerical ones using the direct coding technique, or display (English One-Hot Encoding, OHE). We need quantitative features to be more accurate","metadata":{"id":"V5InfGiWBlBA"}},{"cell_type":"code","source":"data['geography'].value_counts()","metadata":{"id":"hNyfwGfbBlBA","outputId":"c15b5a0c-bfbd-448e-c8ae-9a4c7de3bcb2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['gender'].value_counts()","metadata":{"id":"I5mGLzdRBlBA","outputId":"2212f36f-aff3-4213-97f8-9464a0b1cd05","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OHE of features\ngender_ohe = pd.get_dummies(df[\"gender\"], drop_first=True)\ncountry_ohe = pd.get_dummies(df[\"geography\"], drop_first=True)\n\n# delete catfeatures\ndata.drop([\"gender\", \"geography\"], axis=1, inplace=True)\n\n#concat new sets\ndf_ohe = pd.concat([data, gender_ohe, country_ohe], axis=1)\n\ndf_ohe.head()","metadata":{"id":"cGP-z9YVBlBB","outputId":"98698942-3632-47c9-9285-a474a9364fc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ohe.info()","metadata":{"id":"cnR8yFlMBlBB","outputId":"d199d295-a348-4554-a212-e234c2549397","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns were coded. \n\nIt is also necessary to standardize the characteristics, since the quantitative values ​​vary greatly. We will not apply standardization to the columns `tenure`,` num_of_products`, `has_crcard`,` isactive_member` and to the target with transformed categorical","metadata":{"id":"XhN2fYiQBlBB"}},{"cell_type":"markdown","source":"### Split data set","metadata":{"id":"61LYvcO8BlBC"}},{"cell_type":"markdown","source":"We have prepared features. Now we will divide our samples into training, validation for the selection of hyperparameters and test, on which we will test our model. We will not touch the test sample to the end, we will work out the best model on it","metadata":{"id":"4KTQHZhKBlBC"}},{"cell_type":"code","source":"def split_data(data, target_column):\n    return data.drop(columns=[target_column], axis=1), data[target_column]","metadata":{"id":"Lmvvec2NBlBC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features, target = split_data(df_ohe,'exited')","metadata":{"id":"Ri8aprjfBlBC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get a validation sample of 20% and divide the remaining 80% again to obtain a test sample. We will conduct training on 60% of the data","metadata":{"id":"RYQH87RSBlBC"}},{"cell_type":"code","source":"features_df, features_valid, target_df, target_valid = ( \n                                train_test_split(\n                                features, target, test_size=0.20, random_state=42)\n)\n","metadata":{"id":"zyHYCfj_BlBC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_train, features_test, target_train, target_test = ( \n                                train_test_split(\n                                features_df, target_df, test_size=0.25, random_state=42)\n)\n","metadata":{"id":"AkbZkLFjBlBC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Objects of train:', len(features_train))\nprint('Objects of valid:', len(features_valid))\nprint('Objects of test:', len(features_test))\nprint('Sum of objects:', len(features_train) + len(features_test) + len(features_test))\nprint()\nprint('Objects of original set (check sum):', len(df_ohe))","metadata":{"id":"gfADQLbiBlBC","outputId":"e6900da9-890d-45c5-916e-bdfa09a865c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sample was divided, we can proceed to trial training of the models. In our task, there is a strong class imbalance, which has a bad effect on training the model. Let's look at the results, we will evaluate the model by the F1 measure - it is a good candidate for a formal metric for assessing the quality of the classifier. It reduces to one number two other fundamental metrics: `precision` and` recall`","metadata":{"id":"wQGJCY-JBlBD"}},{"cell_type":"markdown","source":"### Scaling","metadata":{"id":"jZ-7XE2ADbFH"}},{"cell_type":"markdown","source":"Scaling features across the entire dataset can lead to a data leak. You only need to train the scaler on the train.\n\nWe will train and then apply to our samples","metadata":{"id":"kF-NoHuzDSIQ"}},{"cell_type":"code","source":"numeric = ['creditscore', 'age', 'balance', 'estimated_salary']\nscaler = StandardScaler()\nscaler.fit(features_train[numeric])\npd.options.mode.chained_assignment = None\nfeatures_train[numeric] = scaler.transform(features_train[numeric])\nfeatures_train.head()","metadata":{"scrolled":true,"id":"5fMSRArQDSId","outputId":"c28dd70f-fd4d-43db-d84d-a0fca898930a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply a trained scaller to the validation set","metadata":{"id":"0HKws3zvEJIN"}},{"cell_type":"code","source":"features_valid[numeric] = scaler.transform(features_valid[numeric])\nfeatures_valid.head()","metadata":{"id":"lGf7249TDx_k","outputId":"e5921416-4459-4c46-8ffa-fd8f5573f273","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply a trained scaller to the test set","metadata":{"id":"v0JrOGKDETL7"}},{"cell_type":"code","source":"features_test[numeric] = scaler.transform(features_test[numeric])\nfeatures_test.head()","metadata":{"id":"3sLuclFMD63u","outputId":"acc43784-a3a5-4aa2-e984-b39de494f4f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trial training of models without considering class imbalance","metadata":{"id":"AdmRuLFJBlBD"}},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{"id":"KXnZKx0WBlBD"}},{"cell_type":"markdown","source":"Let's start with basic logistic regression. We do not indicate the weight of the classes","metadata":{"id":"HULjf__0BlBD"}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"FbxhaPthBlBD","outputId":"1cf936be-6b41-45b4-9859-2b5e998ee4d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nPoor enough indicator. Let's try to specify `class_weight = 'balanced' '","metadata":{"id":"dvCo4YtuBlBD"}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"LxMotO4RBlBE","outputId":"314de04f-10a9-490c-9d4d-1935f453de76","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Better now. At this stage, we will not select the hyperparameters, we will move on to the next algorithm","metadata":{"id":"KUKGnVAcBlBE"}},{"cell_type":"markdown","source":"#### Random forest","metadata":{"id":"odBPdiJCBlBE"}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"Crwx3_GlBlBE","outputId":"4f864aa1-4a31-4d9d-9a1b-a5716da25c0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The random forest did better in terms of class imbalance. Similar to logistic regression, let's try setting the `class_weight` parameter","metadata":{"id":"LA4JKci-BlBE"}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10, class_weight='balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"AGHK87iNBlBE","outputId":"449273d0-fc68-43ec-d071-aa8ff9408c8d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The indicator has worsened. also now we will not change the hyperparameters, we will return to this after we fix the imbalance problem","metadata":{"id":"bR3h35gdBlBE"}},{"cell_type":"markdown","source":"#### Catboost","metadata":{"id":"hSFDixdQBlBE"}},{"cell_type":"code","source":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"81EI8qtcBlBF","outputId":"439de96a-ccb4-42ee-85b0-df4861d77051","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"F1 is a fairly high measure, let's look at the results after we select the hyperparameters and test the model on a test sample","metadata":{"id":"8IML3MLyBlBF"}},{"cell_type":"markdown","source":"### Conclusion","metadata":{"id":"GgDLmtP1BlBF"}},{"cell_type":"markdown","source":"We are faced with the task of classification. In order to improve the forecasting results and facilitate the training of the model, we have transformed the data:\n\n- removed unnecessary features - such as surname, customer id and line number\n- carried out coding of categorical variables\n- carried out scaling of quantitative variables\n- divided the samples in a ratio of 60%: 20%: 20% - training, validation for the selection of hyperparameters and model verification, test - for the final model verification and evaluation\n\nWe tried to train the models on objects with class imbalance. Now let's try to get rid of this problem, select the model hyperparameters.","metadata":{"id":"HDy2Z5vmBlBF"}},{"cell_type":"markdown","source":"## Dealing with imbalances and improving models","metadata":{"id":"uD-i-lRdBlBF"}},{"cell_type":"markdown","source":"Classes are not represented in the same way in our problem, let's look again:","metadata":{"id":"eVC3PFMJBlBF"}},{"cell_type":"code","source":"df['exited'].value_counts().to_frame()","metadata":{"id":"his-n0s4BlBF","outputId":"63494028-a593-4049-a993-00de14f99c0a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to solve this problem in three ways. We will choose the best one and use it to improve our model.","metadata":{"id":"4yx64HseBlBF"}},{"cell_type":"markdown","source":"### Upsampling","metadata":{"id":"h1Yl5qXbBlBG"}},{"cell_type":"markdown","source":"To do this, let's use a function that performs the following transformations:\n\n- divide the training sample into negative and positive objects\n- copy positive objects several times\n- taking into account the received data, we will create a new training sample\n- shuffle the data ","metadata":{"id":"x1BdjWZABlBG"}},{"cell_type":"code","source":"def upsample(features, target, repeat):\n    \n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n    \n    features_upsampled = shuffle(features_upsampled, random_state=12345)\n    target_upsampled = shuffle(target_upsampled, random_state=12345)\n    \n    return features_upsampled, target_upsampled\n\n    \n    \nfeatures_upsampled, target_upsampled = upsample(features_train, target_train, 5)\n\nprint(features_upsampled.shape)\nprint(target_upsampled.shape)","metadata":{"id":"eHe58QxZBlBG","outputId":"1e8b5a44-3e66-441b-e4f9-50694fd0b034","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{"id":"SLkcDntjBlBG"}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"DNl3pmNPBlBG","outputId":"c9b769da-193b-4fd2-b535-11f31ddb7538","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe a slight increase in the metric, close to the one we got by specifying the `class_weight` parameter","metadata":{"id":"cjA8YqcGBlBG"}},{"cell_type":"markdown","source":"#### Random Forest","metadata":{"id":"tkigeebsBlBG"}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"Sx0RVnK5BlBG","outputId":"bf8ba510-0ef7-4f9f-8670-754c874d64f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is also an improvement here","metadata":{"id":"wyASaWyOBlBH"}},{"cell_type":"markdown","source":"#### Catboost","metadata":{"id":"LHTtRPWmBlBH"}},{"cell_type":"code","source":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"ZeaRIsNGBlBH","outputId":"573b00cd-1e59-494e-f4a8-ddff5a3387bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Catboost Shows Better Results Again On Validation Set\n\nLet's try another way - decreasing the sample","metadata":{"id":"lJyG0lz2BlBH"}},{"cell_type":"markdown","source":"### Downsampling","metadata":{"id":"M7nnkB6bBlBH"}},{"cell_type":"markdown","source":"To do this, let's use a function that performs the following transformations:\n\n- divide the training sample into negative and positive objects\n- randomly discard some of the negative objects\n- taking into account the received data, we will create a new training sample\n- shuffle the data","metadata":{"id":"O9ZynFToBlBH"}},{"cell_type":"code","source":"def downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_sample = features_zeros.sample(frac=0.1, random_state=12345)\n    target_sample = target_zeros.sample(frac=0.1, random_state=12345)\n    \n    features_downsampled = pd.concat([features_sample] + [features_ones])\n    target_downsampled = pd.concat([target_sample] + [target_ones])\n    \n    features_downsampled = shuffle(features_downsampled, random_state=12345)\n    target_downsampled = shuffle(target_downsampled, random_state=12345)\n    \n\n    \n    return features_downsampled, target_downsampled\n\nfeatures_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)\n\nprint(features_downsampled.shape)\nprint(target_downsampled.shape)","metadata":{"id":"J_W6H9mXBlBH","outputId":"df2aa859-85c1-4352-e1f5-a791cacf0752","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Logistic regression","metadata":{"id":"3x4qKBGaBlBH"}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"duKDZ9RMBlBH","outputId":"eb67ada6-3eca-408f-b09e-f2bfe1cc1d5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random forest","metadata":{"id":"AY1w6U6zBlBI"}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"yZMUo3b0BlBI","outputId":"df5b5e1d-3c0b-44db-f54f-26a49a883ca6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Catboost","metadata":{"id":"p2vGnJPeBlBI"}},{"cell_type":"code","source":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))\n\n","metadata":{"id":"Tof6g0-KBlBI","outputId":"c4a434b1-7411-4f03-c84e-926564dc173a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Downsampling` shows worse results than` upsampling` for all three algorithms.\n\nLet's try changing the threshold and see what the metrics will be - this time we'll turn to `recall` and` precision`","metadata":{"id":"85LNO44iBlBI"}},{"cell_type":"markdown","source":"### Change threshold","metadata":{"id":"5F1k7Lg-BlBI"}},{"cell_type":"markdown","source":"For convenience, we will translate the proximity to the classes into the probability of classes (we have two classes - 0 and 1). The probability of class \"1\" is enough for us. By default it is equal to 0.5 - let's try different parameters, for example, up to 0.95","metadata":{"id":"PagerJKeBlBI"}},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{"id":"i5FPaB4SBlBI"}},{"cell_type":"code","source":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_train, target_train)\nprobabilities_valid = model.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(target_valid, predicted_valid)\n    recall = recall_score(target_valid, predicted_valid)\n    f1 = f1_score(target_valid, predicted_valid)\n    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f} | F1-score = {:.3f}\".format(\n        threshold, precision, recall, f1))\n\nprecision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])    \nplt.figure(figsize=(10, 10))\nplt.step(recall, precision, where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('PR curve')\nplt.show() ","metadata":{"id":"8Afk3GesBlBI","outputId":"3e4878a7-5bf4-4ad2-fbb5-ecd8aef3b22d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a threshold of 0, the completeness is 1 - all answers are positive. At a threshold of 0.85, the model stops giving correct answers. The highest F1 value is observed with a threshold of 0.25","metadata":{"id":"LytUC0-xBlBJ"}},{"cell_type":"markdown","source":"#### Random forest","metadata":{"id":"SzPaDANhBlBJ"}},{"cell_type":"code","source":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_train, target_train)\nprobabilities_valid = model.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(target_valid, predicted_valid)\n    recall = recall_score(target_valid, predicted_valid)\n    f1 = f1_score(target_valid, predicted_valid)\n    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f} | F1-score = {:.3f}\".format(\n        threshold, precision, recall, f1))\n\nprecision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])    \nplt.figure(figsize=(10, 10))\nplt.step(recall, precision, where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('PR curve')\nplt.show() ","metadata":{"id":"f31WTiwhBlBJ","outputId":"68dc848a-473b-44b5-d9b5-774233ea39b4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest indicator is reached at a threshold of 0.2","metadata":{"id":"aQFroQAYBlBJ"}},{"cell_type":"markdown","source":"Let's choose upsampling increasing the sample. On it we will train our models and select the hyperparameters. We will not change the threshold or reduce the sample","metadata":{"id":"_jNK0dCiBlBJ"}},{"cell_type":"markdown","source":"### Train Models and Tuning Hyperparameters","metadata":{"id":"lFm3rrapBlBJ"}},{"cell_type":"markdown","source":"We will train the model on an enlarged sample, check the parameters on a validation sample and evaluate it by the F1-measure, we will not use cross-validation for logistic regression and a random forest.\n\nThe parameters will be selected through `GridSearchCV`. loop and enumeration will not be used","metadata":{"id":"ShcKO72UBlBJ"}},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{"id":"1q2oi_qXBlBJ"}},{"cell_type":"code","source":"par_grid_logist = {\n                   'intercept_scaling': [0.5, 1.0, 1.5],\n                   'class_weight': [None, 'balanced'],\n                   'C': [0.5, 1, 1.5]\n                   }\nmodel = LogisticRegression(solver='liblinear',random_state=42)\n\ngrid_search = GridSearchCV(model, par_grid_logist, cv=5,\n                           scoring='f1')\ngrid_search.fit(features_upsampled, target_upsampled)","metadata":{"id":"TKGxVFwWBlBK","outputId":"11131054-1886-4d28-cbca-86a952cce452","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"id":"AokzuulgBlBK","outputId":"5c27f47a-24c4-45d8-8659-2b93d38b1e37","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply our parameters and see the result:","metadata":{"id":"3DTofTPhBlBK"}},{"cell_type":"code","source":"model_lreg = LogisticRegression(C=1.5, class_weight=None, intercept_scaling=0.5,\n                                solver='liblinear', random_state=42\n)\nmodel_lreg.fit(features_upsampled, target_upsampled)\npredicted_valid = model_lreg.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))\n","metadata":{"id":"7GuCJNZgBlBK","outputId":"62dd55c9-7737-42af-8526-aa7c5506dc79","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below the threshold of 0.59, let's see how the model will behave during testing","metadata":{"id":"FLQb50psBlBK"}},{"cell_type":"code","source":"probabilities_valid = model_lreg.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","metadata":{"id":"219b9RvrBlBK","outputId":"9e52597a-ce95-41b3-f6a3-2ef428370e9c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AUC greater than 0.5, our model is better than random","metadata":{"id":"zXqCfj28BlBK"}},{"cell_type":"markdown","source":"#### Random forest","metadata":{"id":"-0rEIhj_BlBK"}},{"cell_type":"code","source":"par_grid_ensemble = {'n_estimators': [3, 10, 30],\n                     'criterion': ['gini', 'entropy'],\n                     'min_samples_split': range(5, 15)\n                    }\nmodel = RandomForestClassifier(random_state=42)\n\ngrid_search = GridSearchCV(model, par_grid_ensemble, cv=5,\n                           scoring='accuracy'\n                          )\ngrid_search.fit(features_upsampled, target_upsampled)","metadata":{"id":"QnHJUXonBlBL","outputId":"a14c8563-0074-4f5d-f375-758da5187f89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"id":"CERTPRRuBlBL","outputId":"a49f3b7e-46cf-437d-d766-b62d4e00a6ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_rfc = RandomForestClassifier(random_state=42, criterion='gini', \n                               min_samples_split=5, n_estimators=30\n                              )\nmodel_rfc.fit(features_upsampled, target_upsampled)\npredicted_valid = model_rfc.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","metadata":{"id":"tGFZchs8BlBL","outputId":"dc06bfce-da60-4ac8-d3bd-ed073be076d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above the threshold of 0.59 on the validation set. Let's try on a sample test and see how the model behaves on unfamiliar data","metadata":{"id":"rEficog7BlBL"}},{"cell_type":"code","source":"probabilities_valid = model_rfc.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","metadata":{"id":"aR7Qqc5pBlBL","outputId":"28d91171-3af9-4046-c2b4-7a876a712515","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The AUC also tells us that the model is better, random, and better than logistic regression.","metadata":{"id":"EE3yOOQaBlBL"}},{"cell_type":"markdown","source":"#### Catboost (bonus)","metadata":{"id":"knTbpIxaBlBL"}},{"cell_type":"markdown","source":"Let's try to configure Catboost using cross-validation. We will get the basic model, we will check it by the F1-score. Excluding class imbalance","metadata":{"id":"XaeSaYx0BlBM"}},{"cell_type":"code","source":"model_cat = CatBoostClassifier(\n                           custom_loss=['F1'],\n                           random_seed=42,\n                           logging_level='Silent'\n)","metadata":{"id":"AR_B-1pZBlBM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cat.fit(\n          features_train, target_train,\n          eval_set=(features_valid, target_valid)\n\n)","metadata":{"id":"qi14KbuzBlBM","outputId":"22ccd790-fa9b-40ec-c272-8d7efc1751fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the grid of parameters and cross-validate using the built-in Pool function","metadata":{"id":"SMDE5XVLBlBM"}},{"cell_type":"code","source":"cv_params = model_cat.get_params()\ncv_params.update({\n                 'loss_function': 'Logloss'\n})\ncv_data = cv(\n             Pool(features_train, target_train),\n             cv_params\n)","metadata":{"id":"g8Yg39EyBlBM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('F1-score: {}'.format(np.max(cv_data['test-F1-mean'])))","metadata":{"id":"kTFp0yyaBlBM","outputId":"a9d682d0-c7fd-4221-bdc1-50296451fe0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities_valid = model_cat.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","metadata":{"id":"nh3R7PVPBlBM","outputId":"e30c8ad9-884b-4c4c-f528-4b0a668330ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got a very good result, let's see how the model will behave on the test set. Model is better than random","metadata":{"id":"18-eUgMlBlBM"}},{"cell_type":"markdown","source":"## Model testing and validation","metadata":{"id":"bJCrHoltBlBN"}},{"cell_type":"code","source":"# collect indicators in lists\n\ntable_of_model = []\ntable_of_prec = []\ntable_of_acc = []","metadata":{"id":"1IiKv3fdBlBN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Models","metadata":{"id":"RLGU5gkABlBN"}},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{"id":"_FNjSBLnBlBN"}},{"cell_type":"code","source":"predictions_test = model_lreg.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-мера\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('LogisticRegression')","metadata":{"id":"uNyjZSUSBlBN","outputId":"b2cb868c-779a-4565-83a7-a99c9bfd955d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random forest","metadata":{"id":"a2ZmgPQIBlBN"}},{"cell_type":"code","source":"predictions_test = model_rfc.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-мера\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('RandomForestClassifier')","metadata":{"id":"ERx6K32BBlBN","outputId":"984603f7-3792-4d6e-a519-ac4cf9991d77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_rfc.feature_importances_","metadata":{"id":"8_7IQ4XrBlBO","outputId":"add1e790-8cc5-4a55-d3ea-66ef6b4d1b1d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_test.columns","metadata":{"id":"DLCxa30RBlBO","outputId":"4afd38ab-bfad-4f31-9559-2328aad53126","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = pd.DataFrame({'name':features_test.columns,'fi':model_rfc.feature_importances_})\nfi.sort_values('fi',ascending=False)","metadata":{"id":"XmT_w3LzBlBO","outputId":"137720e3-fccf-407e-eac2-8a32e4e93fa9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Catboost","metadata":{"id":"yNEEKBgDBlBO"}},{"cell_type":"code","source":"predictions = model_cat.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-мера\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('Catboost')","metadata":{"id":"Fdh7-K5RBlBO","outputId":"4f57cbbb-e89c-4f5c-b2a7-b7fadafa13c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cat.feature_importances_","metadata":{"id":"tosByFEHBlBO","outputId":"af54cc99-8138-4401-a58e-14d739a8c988","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi_cat = pd.DataFrame({'name':features_test.columns,'fi_cat':model_cat.feature_importances_})\nfi_cat.sort_values('fi_cat',ascending=False)","metadata":{"scrolled":true,"id":"-v1wfqAgBlBO","outputId":"31d0ea8f-3e7c-428e-abe9-14f5a044bdad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conclusion","metadata":{"id":"f0C8v4uaBlBP"}},{"cell_type":"markdown","source":"\nFor convenience, we will display a table of our parameters by model:","metadata":{"id":"EuqeMnCuBlBP"}},{"cell_type":"code","source":"table_of_models = (pd.DataFrame({'Model':table_of_model, 'Accuracy':table_of_acc, \n                                'F1 score':table_of_prec}).sort_values(by='F1 score', ascending=False).\n                  reset_index(drop=True))\ntable_of_models['Threshold of testing'] = (\n                   table_of_models['F1 score'].apply(lambda x: 'good model' if x>0.59 else 'bad model')\n)\ntable_of_models","metadata":{"id":"ZSBEjHEUBlBP","outputId":"8b6cf2fc-a9ac-4466-ec97-959d25fbfb01","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best result was obtained on the Random forest - 0.61, Catboost takes the second place - but this is without correcting the imbalance problem! Logistic regression could not overcome the F1-score threshold of 0.59\n\nWe also looked at what features are important for classification models:\nage, expected salary, credit rate, balance and number of products - age is in the lead","metadata":{"id":"NZDVP_tBBlBP"}},{"cell_type":"markdown","source":"### Senity test","metadata":{"id":"qGCW7i58BlBP"}},{"cell_type":"markdown","source":"#### Comparison with constant","metadata":{"id":"MD1cg7HkBlBP"}},{"cell_type":"markdown","source":"Let's compare our models with a constant model: it predicts class \"0\" for any object","metadata":{"id":"xp1qW79EBlBP"}},{"cell_type":"code","source":"target_const = target*0\nacc_const = accuracy_score(target, target_const)\n\n\nprint(\"Accuracy\")\nprint(\"const:\", acc_const)\n","metadata":{"id":"NC3pohwIBlBP","outputId":"7f7e916c-a7a0-4169-cc9c-b02c9659376c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest and Catboost have been validated. The accuracy of our models is higher than that of the random one. We also looked at ROC-AUC validations, our models performed better.","metadata":{"id":"scJ2haACBlBP"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"OFPbTUxRBlBQ"}},{"cell_type":"markdown","source":"We were provided with historical data on customer behavior and termination of agreements with the bank. Based on this data, we formed features for training the model in order to predict customer churn. We have achieved the best results with a model based on the Random Forest algorithm - F1 measure - `0.61`.\n\nBased on the analysis (using the best model as an example):","metadata":{"id":"5CrerqzrBlBQ"}},{"cell_type":"code","source":"fi.sort_values('fi',ascending=False).reset_index(drop=True).head()","metadata":{"id":"26vScDkKBlBQ","outputId":"4d3ae911-da6b-4280-89bd-30632f475f3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important signs to look out for are:\n\n - client's age\n - credit speed\n - expected profit\n - account balance\n - number of products\n \n \n To predict churn, you can use a model based on the Random Forest algorithm","metadata":{"id":"DROEiUOMBlBQ"}}]}